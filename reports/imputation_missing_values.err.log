Traceback (most recent call last):
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/manuel/Documents/github/FIM/.venv/lib/python3.12/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from fim.models.imputation import FIMImputationWindowed
import torch
model = FIMImputationWindowed.from_pretrained("FIM4Science/fim-windowed-imputation")
base_model=model.fim_imputation.fim_base
------------------


[31m---------------------------------------------------------------------------[39m
[31mAttributeError[39m                            Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[1][39m[32m, line 3[39m
[32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mfim[39;00m[34;01m.[39;00m[34;01mmodels[39;00m[34;01m.[39;00m[34;01mimputation[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m FIMImputationWindowed
[32m      2[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mtorch[39;00m
[32m----> [39m[32m3[39m model = [43mFIMImputationWindowed[49m[43m.[49m[43mfrom_pretrained[49m[43m([49m[33;43m"[39;49m[33;43mFIM4Science/fim-windowed-imputation[39;49m[33;43m"[39;49m[43m)[49m
[32m      4[39m base_model=model.fim_imputation.fim_base

[36mFile [39m[32m~/Documents/github/FIM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4096[39m, in [36mPreTrainedModel.from_pretrained[39m[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)[39m
[32m   4090[39m     config = [38;5;28mcls[39m._autoset_attn_implementation(
[32m   4091[39m         config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map
[32m   4092[39m     )
[32m   4094[39m [38;5;28;01mwith[39;00m ContextManagers(init_contexts):
[32m   4095[39m     [38;5;66;03m# Let's make sure we don't run the init function of buffer modules[39;00m
[32m-> [39m[32m4096[39m     model = [38;5;28;43mcls[39;49m[43m([49m[43mconfig[49m[43m,[49m[43m [49m[43m*[49m[43mmodel_args[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mmodel_kwargs[49m[43m)[49m
[32m   4098[39m [38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it[39;00m
[32m   4099[39m config = model.config

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/imputation.py:494[39m, in [36mFIMImputationWindowed.__init__[39m[34m(self, config, **kwargs)[39m
[32m    491[39m [38;5;28msuper[39m(FIMImputationWindowed, [38;5;28mself[39m).[34m__init__[39m(config=config, **kwargs)
[32m    492[39m [38;5;28mself[39m.logger = RankLoggerAdapter(logging.getLogger([38;5;28mself[39m.[34m__class__[39m.[34m__name__[39m))
[32m--> [39m[32m494[39m [38;5;28;43mself[39;49m[43m.[49m[43m_create_model[49m[43m([49m[43m)[49m

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/imputation.py:504[39m, in [36mFIMImputationWindowed._create_model[39m[34m(self)[39m
[32m    501[39m     [38;5;28mself[39m.denoising_model = [38;5;28;01mlambda[39;00m x, mask: x
[32m    503[39m [38;5;66;03m# self.fim_imputation: FIMImputation = load_model_from_checkpoint(config.fim_imputation, module=FIMImputation)[39;00m
[32m--> [39m[32m504[39m [38;5;28mself[39m.fim_imputation = [43mFIMImputation[49m[43m.[49m[43mfrom_pretrained[49m[43m([49m[43mconfig[49m[43m.[49m[43mfim_imputation[49m[43m)[49m

[36mFile [39m[32m~/Documents/github/FIM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4096[39m, in [36mPreTrainedModel.from_pretrained[39m[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)[39m
[32m   4090[39m     config = [38;5;28mcls[39m._autoset_attn_implementation(
[32m   4091[39m         config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map
[32m   4092[39m     )
[32m   4094[39m [38;5;28;01mwith[39;00m ContextManagers(init_contexts):
[32m   4095[39m     [38;5;66;03m# Let's make sure we don't run the init function of buffer modules[39;00m
[32m-> [39m[32m4096[39m     model = [38;5;28;43mcls[39;49m[43m([49m[43mconfig[49m[43m,[49m[43m [49m[43m*[49m[43mmodel_args[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mmodel_kwargs[49m[43m)[49m
[32m   4098[39m [38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it[39;00m
[32m   4099[39m config = model.config

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/imputation.py:63[39m, in [36mFIMImputation.__init__[39m[34m(self, config, **kwargs)[39m
[32m     45[39m [38;5;66;03m# self._device_map = device_map[39;00m
[32m     46[39m [38;5;66;03m# self._torch_dtype = None[39;00m
[32m     47[39m [38;5;66;03m# self._quantization_config = None[39;00m
[32m   (...)[39m[32m     58[39m [38;5;66;03m# if use_bf16 and is_bfloat_supported:[39;00m
[32m     59[39m [38;5;66;03m#     self._torch_dtype = torch.float16[39;00m
[32m     61[39m [38;5;28mself[39m.loc_normalize_locations = MinMaxNormalization()
[32m---> [39m[32m63[39m [38;5;28;43mself[39;49m[43m.[49m[43m_create_model[49m[43m([49m[43m)[49m

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/imputation.py:67[39m, in [36mFIMImputation._create_model[39m[34m(self)[39m
[32m     65[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_create_model[39m([38;5;28mself[39m):
[32m     66[39m     config = deepcopy([38;5;28mself[39m.config)
[32m---> [39m[32m67[39m     [38;5;28mself[39m.fim_base = [43mFIMODE[49m[43m.[49m[43mfrom_pretrained[49m[43m([49m[43mconfig[49m[43m.[49m[43mfim_base[49m[43m)[49m
[32m     68[39m     [38;5;66;03m# self.fim_base: FIMODE = load_model_from_checkpoint(config.fim_base, module=FIMODE)[39;00m
[32m     70[39m     [38;5;28mself[39m.fim_base.apply_normalization = config.use_fim_normalization

[36mFile [39m[32m~/Documents/github/FIM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4096[39m, in [36mPreTrainedModel.from_pretrained[39m[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)[39m
[32m   4090[39m     config = [38;5;28mcls[39m._autoset_attn_implementation(
[32m   4091[39m         config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map
[32m   4092[39m     )
[32m   4094[39m [38;5;28;01mwith[39;00m ContextManagers(init_contexts):
[32m   4095[39m     [38;5;66;03m# Let's make sure we don't run the init function of buffer modules[39;00m
[32m-> [39m[32m4096[39m     model = [38;5;28;43mcls[39;49m[43m([49m[43mconfig[49m[43m,[49m[43m [49m[43m*[49m[43mmodel_args[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mmodel_kwargs[49m[43m)[49m
[32m   4098[39m [38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it[39;00m
[32m   4099[39m config = model.config

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/ode.py:60[39m, in [36mFIMODE.__init__[39m[34m(self, config, **kwargs)[39m
[32m     57[39m [38;5;28;01melse[39;00m:
[32m     58[39m     [38;5;28mself[39m.apply_normalization = [38;5;28;01mTrue[39;00m
[32m---> [39m[32m60[39m [38;5;28;43mself[39;49m[43m.[49m[43m_create_model[49m[43m([49m[43m)[49m

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/ode.py:72[39m, in [36mFIMODE._create_model[39m[34m(self)[39m
[32m     68[39m [38;5;28mself[39m.time_encoding = create_class_instance(config.time_encoding.pop([33m"[39m[33mname[39m[33m"[39m), config.time_encoding)
[32m     70[39m [38;5;28mself[39m.trunk_net = create_class_instance(config.trunk_net.pop([33m"[39m[33mname[39m[33m"[39m), config.trunk_net)
[32m---> [39m[32m72[39m [38;5;28mself[39m.branch_net = [43mcreate_class_instance[49m[43m([49m[43mconfig[49m[43m.[49m[43mbranch_net[49m[43m.[49m[43mpop[49m[43m([49m[33;43m"[39;49m[33;43mname[39;49m[33;43m"[39;49m[43m)[49m[43m,[49m[43m [49m[43mconfig[49m[43m.[49m[43mbranch_net[49m[43m)[49m
[32m     74[39m [38;5;28;01mif[39;00m config.combiner_net.get([33m"[39m[33min_features[39m[33m"[39m) != [32m2[39m * config.combiner_net.get([33m"[39m[33mout_features[39m[33m"[39m):
[32m     75[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mThe number of input features for the combiner_net must be twice the number of output features (latent dim).[39m[33m"[39m)

[36mFile [39m[32m~/Documents/github/FIM/src/fim/utils/helper.py:98[39m, in [36mcreate_class_instance[39m[34m(class_full_path, kwargs, *args)[39m
[32m     96[39m     instance = clazz(*args)
[32m     97[39m [38;5;28;01melse[39;00m:
[32m---> [39m[32m98[39m     instance = [43mclazz[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    100[39m [38;5;28;01mreturn[39;00m instance

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/blocks/base.py:270[39m, in [36mTransformer.__init__[39m[34m(self, num_encoder_blocks, dim_model, dim_time, num_heads, dropout, residual_mlp, batch_first)[39m
[32m    264[39m [38;5;28mself[39m.num_heads = num_heads
[32m    266[39m [38;5;28mself[39m.input_projection = nn.Linear(dim_time + [32m1[39m, dim_model)
[32m    268[39m [38;5;28mself[39m.encoder_blocks = nn.ModuleList(
[32m    269[39m     [
[32m--> [39m[32m270[39m         [43mEncoderBlock[49m[43m([49m[43mdim_model[49m[43m,[49m[43m [49m[43mnum_heads[49m[43m,[49m[43m [49m[43mdropout[49m[43m,[49m[43m [49m[43mcopy[49m[43m.[49m[43mdeepcopy[49m[43m([49m[43mresidual_mlp[49m[43m)[49m[43m,[49m[43m [49m[43mbatch_first[49m[43m=[49m[43mbatch_first[49m[43m)[49m
[32m    271[39m         [38;5;28;01mfor[39;00m _ [38;5;129;01min[39;00m [38;5;28mrange[39m(num_encoder_blocks - [32m1[39m)
[32m    272[39m     ]
[32m    273[39m )
[32m    275[39m [38;5;28mself[39m.final_query_vector = nn.Parameter(torch.randn([32m1[39m, [32m1[39m, dim_model))
[32m    276[39m [38;5;28mself[39m.final_attention = nn.MultiheadAttention(dim_model, num_heads, dropout=dropout, batch_first=batch_first)

[36mFile [39m[32m~/Documents/github/FIM/src/fim/models/blocks/base.py:354[39m, in [36mEncoderBlock.__init__[39m[34m(self, d_model, num_heads, dropout, residual_mlp, batch_first)[39m
[32m    351[39m [38;5;28mself[39m.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=batch_first)
[32m    352[39m [38;5;28mself[39m.layer_norm1 = nn.LayerNorm(d_model)
[32m    353[39m [38;5;28mself[39m.residual_mlp = create_class_instance(
[32m--> [39m[32m354[39m     [43mresidual_mlp[49m[43m.[49m[43mpop[49m([33m"[39m[33mname[39m[33m"[39m),
[32m    355[39m     residual_mlp,
[32m    356[39m )
[32m    357[39m [38;5;28mself[39m.layer_norm2 = nn.LayerNorm(d_model)

[36mFile [39m[32m~/Documents/github/FIM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1962[39m, in [36mModule.__getattr__[39m[34m(self, name)[39m
[32m   1960[39m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[32m   1961[39m         [38;5;28;01mreturn[39;00m modules[name]
[32m-> [39m[32m1962[39m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m(
[32m   1963[39m     [33mf[39m[33m"[39m[33m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m).[34m__name__[39m[38;5;132;01m}[39;00m[33m'[39m[33m object has no attribute [39m[33m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m'[39m[33m"[39m
[32m   1964[39m )

[31mAttributeError[39m: 'MLP' object has no attribute 'pop'


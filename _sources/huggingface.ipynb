{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the FIM-MJP Model \n",
    "\n",
    "## Input to the model\n",
    "\n",
    "The model takes as an input dictionary containing at least three items and one additional argument. The input dictionary should contain the following items:\n",
    "\n",
    "   1. The _observation grid_ with size `[num_paths, grid_size]` which are the locations in time when a observation was recorded. The key in the dictionary is `observation_grid` and the data type is `float`.\n",
    "   2. The _observation values_ with size `[num_paths, grid_size]` are the actually observed values (state) of the process. The key in the dictionary is `observation_values` and the data type is `int`.\n",
    "   3. The _sequence length_ with size `[num_paths]` which is the length of the observed sequence. The key in the dictionary is `seq_length` and the data type is `int`.\n",
    "   4. The _dimension of the process_ which is an `integer` between 2 and 6. The maximum number of states that are supported by our model is 6. The argument name is `n_states`.\n",
    "\n",
    "Optionally, the dictionary can contain the following items:\n",
    "\n",
    "   - The _time normalization factor_ with size `[num_paths]` which is the factor by which the time is normalized. The key in the dictionary is `time_normalization_factors` and the data type is `float`. In case this item is not provided, the model will normalize the time by the maximum time in the observation grid.\n",
    "   - Items for calculating the loss:\n",
    "      - _intensity matrix_ with size `[num_paths, n_states, n_states]` which is the intensity matrix of the process. The key in the dictionary is `intensity_matrices` and the data type is `float`.\n",
    "      - _initial distribution_ with size `[num_paths, n_states]` which is the initial distribution of the process. The key in the dictionary is `initial_distributions` and the data type is `int`.\n",
    "      - _adjacency matrix_ with size `[num_paths, n_states, n_states]` which is the adjacency matrix of the process. The key in the dictionary is `adjacency_matrices` and the data type is `int`.\n",
    "\n",
    "### Output of the model\n",
    "\n",
    "The model returns a dictionary containing the following items:\n",
    "\n",
    "   - The _intensity matrix_ with size `[num_paths, n_states, n_states]` which is the intensity matrix of the process. The key in the dictionary is `intensity_matrices` and the data type is `float`.\n",
    "   - The _initial distribution_ with size `[num_paths, n_states]` which is the initial distribution of the process. The key in the dictionary is `initial_distributions` and the data type is `int`.\n",
    "   - The _adjacency matrix_ with size `[num_paths, n_states, n_states]` which is the adjacency matrix of the process. The key in the dictionary is `adjacency_matrices` and the data type is `int`.\n",
    "   - The _losses_ which is the loss of the model. The key in the dictionary is `loss` and the data type is `float`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from fim.trainers.utils import get_accel_type\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cpu\")#get_accel_type() due to current issues with cuda and the RTX 5090 / 570 driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We also provide a synthetic dataset for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e69da88b62e425eb5440067bff78546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/V1.json:   0%|          | 0.00/27.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d759676c8f4fd98feeefefc26b156f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Discrete Flashing Ratchet (DFR) dataset from Huggingface\n",
    "data = load_dataset(\"FIM4Science/mjp\", download_mode=\"force_redownload\", name=\"default\")\n",
    "data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RNNBase.__init__() missing 1 required positional argument: 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Loading the FIMMJP model from Huggingface\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fimmjp = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFIM4Science/fim-mjp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m fimmjp = fimmjp.to(device)\n\u001b[32m      4\u001b[39m fimmjp.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/OpenFIM/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    558\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/OpenFIM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4096\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4090\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._autoset_attn_implementation(\n\u001b[32m   4091\u001b[39m         config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n\u001b[32m   4092\u001b[39m     )\n\u001b[32m   4094\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[32m   4095\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4096\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4098\u001b[39m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[32m   4099\u001b[39m config = model.config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/FIM4Science/fim-mjp/89090c4630461cfa7376e96836642d053f0a5407/mjp.py:128\u001b[39m, in \u001b[36mFIMMJP.__init__\u001b[39m\u001b[34m(self, config, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mself\u001b[39m.gaussian_nll = nn.GaussianNLLLoss(full=\u001b[38;5;28;01mTrue\u001b[39;00m, reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mself\u001b[39m.init_cross_entropy = nn.CrossEntropyLoss(reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__create_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/FIM4Science/fim-mjp/89090c4630461cfa7376e96836642d053f0a5407/mjp.py:142\u001b[39m, in \u001b[36mFIMMJP.__create_modules\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mself\u001b[39m.pos_encodings = create_class_instance(pos_encodings.pop(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m), pos_encodings)\n\u001b[32m    141\u001b[39m ts_encoder[\u001b[33m\"\u001b[39m\u001b[33min_features\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.config.n_states + \u001b[38;5;28mself\u001b[39m.pos_encodings.out_features\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28mself\u001b[39m.ts_encoder = \u001b[43mcreate_class_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m.path_attention = create_class_instance(path_attention.pop(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m), path_attention)\n\u001b[32m    146\u001b[39m in_features = intensity_matrix_decoder.get(\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33min_features\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.ts_encoder.out_features + ((\u001b[38;5;28mself\u001b[39m.total_offdiagonal_transitions + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_adjacency_matrix \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m),\n\u001b[32m    149\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/FIM/src/fim/utils/helper.py:91\u001b[39m, in \u001b[36mcreate_class_instance\u001b[39m\u001b[34m(class_full_path, kwargs, *args)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         kwargs[key] = \u001b[43mcreate_class_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m module_name, class_name = class_full_path.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     93\u001b[39m module = import_module(module_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/FIM/src/fim/utils/helper.py:98\u001b[39m, in \u001b[36mcreate_class_instance\u001b[39m\u001b[34m(class_full_path, kwargs, *args)\u001b[39m\n\u001b[32m     96\u001b[39m     instance = clazz(*args)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     instance = \u001b[43mclazz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/OpenFIM/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:975\u001b[39m, in \u001b[36mLSTM.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLSTM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: RNNBase.__init__() missing 1 required positional argument: 'input_size'"
     ]
    }
   ],
   "source": [
    "# Loading the FIMMJP model from Huggingface\n",
    "fimmjp = AutoModel.from_pretrained(\"FIM4Science/fim-mjp\", trust_remote_code=True)\n",
    "fimmjp = fimmjp.to(device)\n",
    "fimmjp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data to device\n",
    "batch = {k: v.to(device)[0] for k, v in data[\"train\"][:1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a batch\n",
    "n_paths_eval = [1, 30, 100, 300, 500, 1000, 5000]\n",
    "total_n_paths = batch[\"observation_grid\"].shape[1]\n",
    "statistics = total_n_paths // 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = defaultdict(list)\n",
    "with torch.no_grad():\n",
    "    for n_paths in n_paths_eval:\n",
    "        for _ in range(statistics):\n",
    "            paths_idx = torch.randperm(total_n_paths)[:n_paths]\n",
    "            mini_batch = batch.copy()\n",
    "            mini_batch[\"observation_grid\"] = batch[\"observation_grid\"][:, paths_idx]\n",
    "            mini_batch[\"observation_values\"] = batch[\"observation_values\"][:, paths_idx]\n",
    "            mini_batch[\"seq_lengths\"] = batch[\"seq_lengths\"][:, paths_idx]\n",
    "            output = fimmjp(mini_batch, n_states=6)\n",
    "            result[n_paths].append(output[\"losses\"][\"rmse_loss\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Paths during Evaluation</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.678 ± 0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.300 ± 0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.168 ± 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.121 ± 0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>0.108 ± 0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.226 ± 0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.882 ± 0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Paths during Evaluation           RMSE\n",
       "0                          1  0.678 ± 0.056\n",
       "1                         30  0.300 ± 0.049\n",
       "2                        100  0.168 ± 0.033\n",
       "3                        300  0.121 ± 0.020\n",
       "4                        500  0.108 ± 0.015\n",
       "5                       1000  0.226 ± 0.017\n",
       "6                       5000  0.882 ± 0.001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = {n_paths: torch.tensor(losses).mean().item() for n_paths, losses in result.items()}\n",
    "stds = {n_paths: torch.tensor(losses).std().item() for n_paths, losses in result.items()}\n",
    "\n",
    "df_result = pd.DataFrame(\n",
    "    {\n",
    "        \"# Paths during Evaluation\": list(means.keys()),\n",
    "        \"RMSE\": [f\"{mean:.3f} ± {std:.3f}\" for mean, std in zip(means.values(), stds.values())],\n",
    "    }\n",
    ")\n",
    "\n",
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenFIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
